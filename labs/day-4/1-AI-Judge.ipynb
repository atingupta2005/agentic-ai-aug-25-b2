{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVCjcUqn93B8"
      },
      "source": [
        "# Lab: Building a Model Evaluator with LangGraph\n",
        "\n",
        "This notebook refactors our previous model comparison script into a stateful, robust agent using LangGraph. By structuring the workflow as a graph, we gain better control, observability, and the ability to easily extend the process in the future.\n",
        "\n",
        "### Key Features:\n",
        "1.  **Stateful Agent**: The entire workflow is managed within a `StateGraph`.\n",
        "2.  **Modular Nodes**: Each logical step (generating a question, querying models, judging) is a separate, well-defined node.\n",
        "3.  **Configuration Driven**: Uses a `config.ini` file for settings.\n",
        "4.  **Visualization**: Displays a diagram of the agent's graph structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak6vWkmu93CH"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tt3Z6Jp93CL",
        "outputId": "706fb9c1-d636-4768-cc6d-fb6fd42cb74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (6.17.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.72)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.6.3)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel) (4.3.8)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv openai langgraph ipykernel langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyVI1OvQ93CP"
      },
      "source": [
        "## 2. Imports and Configuration Loading\n",
        "\n",
        "**Purpose**: To import necessary libraries and load all settings from our external `config.ini` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D7cty4193CR",
        "outputId": "f840f262-c6bf-44a7-b87c-8f3a5bb679b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded successfully!\n",
            "Competitor models: ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import configparser\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display, Image\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "# NEW IMPORT for text splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    load_dotenv(\"dev.env\")\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read('1-config.ini')\n",
        "\n",
        "    # Model settings\n",
        "    JUDGE_MODEL = config.get('Models', 'judge_model')\n",
        "    competitor_models_str = config.get('Models', 'competitor_models')\n",
        "    competitor_models = [model.strip() for model in competitor_models_str.split(',')]\n",
        "    \n",
        "    # NEW: Chunk size for large answers\n",
        "    CHUNK_SIZE = config.getint('Parameters', 'chunk_size', fallback=2000)\n",
        "    CHUNK_OVERLAP = config.getint('Parameters', 'chunk_overlap', fallback=200)\n",
        "\n",
        "    print(\"Configuration loaded successfully!\")\n",
        "    print(f\"Competitor models: {competitor_models}\")\n",
        "    print(f\"Chunk Size: {CHUNK_SIZE}, Chunk Overlap: {CHUNK_OVERLAP}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading configuration: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkmbG1mC93CS"
      },
      "source": [
        "## 3. Define the Agent's State\n",
        "\n",
        "**Purpose**: To define the data structure that will act as the agent's memory. This `GraphState` will be passed to every node in our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og2x1rrS93CT"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Represents the state of our model evaluation workflow.\"\"\"\n",
        "    question: str\n",
        "    competitor_models: List[str]\n",
        "    answers: List[dict]\n",
        "    # NEW: key to store chunked answers\n",
        "    chunked_answers: List[dict]\n",
        "    judgement: str\n",
        "    error_message: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg335Nd093CU"
      },
      "source": [
        "## 4. Define the Agent's Nodes\n",
        "\n",
        "**Purpose**: To create the functions that will perform the actual work. Each function takes the current state as input and returns a dictionary with the values to update in the state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_answers_node(state: GraphState):\n",
        "    \"\"\"Chunks long answers to avoid token limit issues.\"\"\"\n",
        "    print(\"--- NODE: Chunking Answers ---\")\n",
        "    answers = state['answers']\n",
        "    chunked_answers = []\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    for answer in answers:\n",
        "        long_answer = answer['answer']\n",
        "        # Only chunk if the answer is long (e.g., > 1000 characters)\n",
        "        if len(long_answer) > CHUNK_SIZE:\n",
        "            chunks = text_splitter.split_text(long_answer)\n",
        "            print(f\"  - Answer from {answer['model']} was chunked into {len(chunks)} parts.\")\n",
        "            # Store chunks in the state along with original metadata\n",
        "            chunked_answers.append({\"model\": answer['model'], \"chunks\": chunks})\n",
        "        else:\n",
        "            # If the answer is short, keep it as a single chunk\n",
        "            chunked_answers.append({\"model\": answer['model'], \"chunks\": [long_answer]})\n",
        "\n",
        "    return {\"chunked_answers\": chunked_answers}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teVHzIlY93CV"
      },
      "outputs": [],
      "source": [
        "def generate_question_node(state: GraphState):\n",
        "    \"\"\"Generates the challenge question.\"\"\"\n",
        "    print(\"--- NODE: Generating Question ---\")\n",
        "    request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation or preamble.\"\n",
        "    messages = [{\"role\": \"user\", \"content\": request}]\n",
        "    try:\n",
        "        client = OpenAI()\n",
        "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages)\n",
        "        question = response.choices[0].message.content\n",
        "        return {\"question\": question}\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to generate question: {e}\"}\n",
        "\n",
        "def query_models_node(state: GraphState):\n",
        "    \"\"\"Queries each competitor model with the challenge question.\"\"\"\n",
        "    print(\"--- NODE: Querying Competitor Models ---\")\n",
        "    question = state['question']\n",
        "    models_to_query = state['competitor_models']\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    current_answers = []\n",
        "    client = OpenAI()\n",
        "\n",
        "    for model_name in models_to_query:\n",
        "        print(f\"  - Querying {model_name}...\")\n",
        "        try:\n",
        "            response = client.chat.completions.create(model=model_name, messages=messages)\n",
        "            answer = response.choices[0].message.content\n",
        "            current_answers.append({\"model\": model_name, \"answer\": answer})\n",
        "        except Exception as e:\n",
        "            error_message = f\"Could not get response from {model_name}: {e}\"\n",
        "            print(f\"    ERROR: {error_message}\")\n",
        "            current_answers.append({\"model\": model_name, \"answer\": error_message})\n",
        "\n",
        "    return {\"answers\": current_answers}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_for_judging_node(state: GraphState):\n",
        "    \"\"\"Chunks long answers and initializes the state for the judging loop.\"\"\"\n",
        "    print(\"--- NODE: Preparing for Judging ---\")\n",
        "    answers = state['answers']\n",
        "    answers_to_rank = []\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    for answer in answers:\n",
        "        long_answer = answer['answer']\n",
        "        chunks = text_splitter.split_text(long_answer)\n",
        "        answers_to_rank.append({\"model\": answer['model'], \"chunks\": chunks})\n",
        "    \n",
        "    return {\n",
        "        \"answers_to_rank\": answers_to_rank,\n",
        "        \"current_model_index\": 0,\n",
        "        \"current_chunk_index\": 0,\n",
        "        \"running_judgment\": \"No initial judgment.\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def final_judge_node(state: GraphState):\n",
        "    \"\"\"Generates the final ranking based on the cumulative judgment memory.\"\"\"\n",
        "    print(\"--- NODE: Final Judge Ranking ---\")\n",
        "    question = state['question']\n",
        "    running_judgment = state['running_judgment']\n",
        "    model_names = [ans['model'] for ans in state['answers_to_rank']]\n",
        "\n",
        "    final_ranking_prompt = (\n",
        "        f\"You have completed a detailed evaluation of multiple AI models' answers. \"\n",
        "        f\"The question was: '{question}'. \"\n",
        "        f\"Your final thoughts and comparative analysis are:\\n<final_assessment_context>{running_judgment}</final_assessment_context>\\n\\n\"\n",
        "        f\"Based on this cumulative assessment, provide the definitive ranking of the models from best to worst. \"\n",
        "        f\"The models were: {', '.join(model_names)}. \"\n",
        "        f\"Respond with a JSON object. The object should have a single key, \\\"results\\\", \"\n",
        "        f\"which is a list of the model names in ranked order. Example: {{\\\"results\\\": [\\\"gpt-4o\\\", \\\"gpt-3.5-turbo\\\"]}}\"\n",
        "    )\n",
        "    messages = [{\"role\": \"user\", \"content\": final_ranking_prompt}]\n",
        "    try:\n",
        "        client = OpenAI()\n",
        "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages, response_format={\"type\": \"json_object\"})\n",
        "        judgement = response.choices[0].message.content\n",
        "        return {\"judgement\": judgement}\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to get final judgement: {e}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_chunk_to_judge_node(state: GraphState):\n",
        "    \"\"\"Sends the next chunk of the current answer to the judge model for evaluation.\"\"\"\n",
        "    print(\"--- NODE: Sending Chunk to Judge ---\")\n",
        "    question = state['question']\n",
        "    running_judgment = state['running_judgment']\n",
        "    answers_to_rank = state['answers_to_rank']\n",
        "    model_index = state['current_model_index']\n",
        "    chunk_index = state['current_chunk_index']\n",
        "    \n",
        "    current_model = answers_to_rank[model_index]['model']\n",
        "    current_chunk = answers_to_rank[model_index]['chunks'][chunk_index]\n",
        "    total_chunks = len(answers_to_rank[model_index]['chunks'])\n",
        "\n",
        "    judge_prompt = (\n",
        "        f\"You are an impartial judge. The question is: '{question}'. \"\n",
        "        f\"You are evaluating the answer from model '{current_model}'. \"\n",
        "        f\"Here is chunk {chunk_index + 1} of {total_chunks}. \"\n",
        "        f\"You may have received previous chunks or other models' answers.\\n\\n\"\n",
        "        f\"Current running judgment:\\n<judgment>{running_judgment}</judgment>\\n\\n\"\n",
        "        f\"New chunk to evaluate:\\n<chunk>{current_chunk}</chunk>\\n\\n\"\n",
        "        f\"Please provide your updated assessment based on all content you have seen so far. \"\n",
        "        f\"Your response should be a single paragraph summary of the updated judgment. Do not provide a final ranking yet.\"\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
        "    try:\n",
        "        client = OpenAI()\n",
        "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages)\n",
        "        updated_judgment = response.choices[0].message.content\n",
        "        return {\"running_judgment\": updated_judgment, \"current_chunk_index\": chunk_index + 1}\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get judgment for chunk: {e}\")\n",
        "        return {\"error_message\": f\"Failed to get judgment for chunk: {e}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def judge_answers_node(state: GraphState):\n",
        "    \"\"\"Uses the judge model to evaluate and rank the chunked answers.\"\"\"\n",
        "    print(\"--- NODE: Judging Answers ---\")\n",
        "    question = state['question']\n",
        "    chunked_answers = state['chunked_answers']\n",
        "\n",
        "    all_answers_text = \"\"\n",
        "    for index, response in enumerate(chunked_answers):\n",
        "        # Concatenate all chunks for a given model\n",
        "        full_response_text = \"\\n[CHUNK_SEPARATOR]\\n\".join(response['chunks'])\n",
        "        all_answers_text += f\"--- START OF RESPONSE {index+1} ({response['model']}) ---\\n{full_response_text}\\n--- END OF RESPONSE {index+1} ---\\n\\n\"\n",
        "\n",
        "    judge_prompt = (\n",
        "        f\"You are an impartial judge in a competition between multiple AI models. \"\n",
        "        f\"Your task is to evaluate each model's response to a question based on clarity, depth, and accuracy. \"\n",
        "        f\"The question was: \\\"{question}\\\". \"\n",
        "        f\"Here are the responses, which may be broken into chunks separated by '[CHUNK_SEPARATOR]'. \"\n",
        "        f\"Please read the entire content of each response. \"\n",
        "        f\"Rank them from best to worst and respond with a JSON object. \"\n",
        "        f\"The object should have a single key, \\\"results\\\", which is a list of the competitor numbers (as integers) in ranked order. \"\n",
        "        f\"Example: {{\\\"results\\\": [2, 1, 3]}}\\n\\n\"\n",
        "        f\"Responses to judge:\\n{all_answers_text}\"\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
        "\n",
        "    try:\n",
        "        client = OpenAI()\n",
        "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages, response_format={\"type\": \"json_object\"})\n",
        "        judgement = response.choices[0].message.content\n",
        "        return {\"judgement\": judgement}\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to get judgement: {e}\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_next_chunk_node(state: GraphState):\n",
        "    answers_to_rank = state['answers_to_rank']\n",
        "    model_index = state['current_model_index']\n",
        "    chunk_index = state['current_chunk_index']\n",
        "    if chunk_index < len(answers_to_rank[model_index]['chunks']):\n",
        "        print(\"--- DECISION: Continue to next chunk for current model ---\")\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        print(\"--- DECISION: All chunks for current model processed ---\")\n",
        "        return \"end_chunk_loop\"\n",
        "\n",
        "def check_next_model_node(state: GraphState):\n",
        "    answers_to_rank = state['answers_to_rank']\n",
        "    model_index = state['current_model_index']\n",
        "    if model_index + 1 < len(answers_to_rank):\n",
        "        print(\"--- DECISION: Continue to next model's answers ---\")\n",
        "        return \"continue\"\n",
        "    else:\n",
        "        print(\"--- DECISION: All models have been judged ---\")\n",
        "        return \"end_all_loops\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aRlkKwt93CW"
      },
      "source": [
        "## 5. Construct and Visualize the Graph\n",
        "\n",
        "**Purpose**: To wire the nodes together into a coherent workflow and visualize the resulting graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "_iEgqFDR93CX",
        "outputId": "53516f33-b9e1-44c1-df11-302701bbcf44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangGraph workflow compiled successfully!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add all nodes to the graph\n",
        "workflow.add_node(\"generate_question\", generate_question_node)\n",
        "workflow.add_node(\"query_models\", query_models_node)\n",
        "workflow.add_node(\"prepare_for_judging\", prepare_for_judging_node)\n",
        "workflow.add_node(\"send_chunk_to_judge\", send_chunk_to_judge_node)\n",
        "workflow.add_node(\"check_next_chunk\", check_next_chunk_node)\n",
        "workflow.add_node(\"check_next_model\", check_next_model_node)\n",
        "workflow.add_node(\"final_judge\", final_judge_node)\n",
        "\n",
        "# Define the overall workflow edges\n",
        "workflow.add_edge(START, \"generate_question\")\n",
        "workflow.add_edge(\"generate_question\", \"query_models\")\n",
        "# The output of query_models now goes to the new prepare node\n",
        "workflow.add_edge(\"query_models\", \"prepare_for_judging\")\n",
        "\n",
        "# Define the looping logic for the iterative judge\n",
        "# 1. Start the loop by sending the first chunk\n",
        "workflow.add_edge(\"prepare_for_judging\", \"send_chunk_to_judge\")\n",
        "\n",
        "# 2. Check if there are more chunks for the current model\n",
        "workflow.add_conditional_edges(\n",
        "    \"send_chunk_to_judge\",\n",
        "    check_next_chunk_node,\n",
        "    {\n",
        "        # If there are more chunks, loop back to send the next one\n",
        "        \"continue\": \"send_chunk_to_judge\",\n",
        "        # If no more chunks, move on to the next model\n",
        "        \"end_chunk_loop\": \"check_next_model\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# 3. Check if there are more models to judge\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_next_model\",\n",
        "    check_next_model_node,\n",
        "    {\n",
        "        # If there are more models, loop back to the start of the chunk loop\n",
        "        \"continue\": \"send_chunk_to_judge\",\n",
        "        # If all models are judged, proceed to final ranking\n",
        "        \"end_all_loops\": \"final_judge\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# After the loop and final judge, the workflow ends\n",
        "workflow.add_edge(\"final_judge\", END)\n",
        "\n",
        "# Compile the graph into a runnable app\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow compiled successfully!\")\n",
        "\n",
        "# Display the graph visualization\n",
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not display graph: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h66LXSNI93CZ"
      },
      "source": [
        "## 6. Execute the Agent Workflow\n",
        "\n",
        "**Purpose**: To run our newly created LangGraph agent. We prepare the initial state and then invoke the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAxpEAb93Ca",
        "outputId": "fb1c1322-1820-4894-fa3b-dfd8d3747b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Invoking Agent Workflow ---\n",
            "--- NODE: Generating Question ---\n",
            "\n",
            "--- Output from Node: 'generate_question' ---\n",
            "{'question': 'How does the concept of identity persistence challenge traditional metaphysical notions of the self, and what implications might this have for the development of artificial intelligence that seeks to mimic human cognitive processes?'}\n",
            "--- NODE: Querying Competitor Models ---\n",
            "  - Querying gpt-3.5-turbo...\n",
            "  - Querying gpt-4o...\n",
            "  - Querying gpt-4o-mini...\n",
            "\n",
            "--- Output from Node: 'query_models' ---\n",
            "{'answers': [{'model': 'gpt-3.5-turbo', 'answer': 'The concept of identity persistence challenges traditional metaphysical notions of the self by questioning the idea of a fixed, unchanging identity. Traditional metaphysical theories often posit a unified and consistent self that continues to exist over time, but identity persistence suggests that our sense of self is constantly evolving and can be influenced by external factors.\\n\\nThis has implications for the development of artificial intelligence that seeks to mimic human cognitive processes, as it raises the question of how to incorporate this dynamic aspect of human identity into AI systems. If the self is not a fixed and unchanging entity, then AI programs designed to replicate human cognitive processes must be capable of adapting and evolving in a similar way.\\n\\nFurthermore, the concept of identity persistence challenges the idea of a distinct boundary between the self and external influences, suggesting that our identity is shaped by our interactions with the world around us. This has implications for AI development, as it suggests that AI systems may need to be able to interact with and learn from their environment in order to develop a more nuanced understanding of human identity and behavior.'}, {'model': 'gpt-4o', 'answer': 'The concept of identity persistence challenges traditional metaphysical notions of the self by questioning the assumption that there is a stable, unchanging core that defines a person\\'s identity over time. Traditional views often rely on the idea of a continuous, singular essence or soul that remains constant throughout a person’s life, providing a foundation for personal identity. In contrast, identity persistence suggests that identity is not static but rather an evolving construct influenced by changing experiences, memories, and perceptions.\\n\\nThis dynamic view of identity emphasizes the role of psychological continuity and memory in maintaining a sense of self over time. It aligns more closely with theories like those of philosopher Derek Parfit, who argued that personal identity is not what matters fundamentally, but rather the connectedness and continuity of psychological states.\\n\\nThe implications for the development of artificial intelligence are profound. If identity is not a fixed essence but a fluid and relational construct, then designing AI to mimic human cognition means creating systems capable of adapting and evolving their \"identity\" in response to new information and experiences. This challenges AI developers to move beyond models that simply replicate human behavior to systems that can authentically embody the complex, changing nature of human identity.\\n\\nMoreover, understanding identity as a flexible construct may lead to more advanced AI that can engage in sophisticated relational interactions, learn contextually from past experiences, and even possess a form of self-awareness that reflects an ongoing narrative rather than a static state. This approach not only enhances the potential for creating AI that can integrate more seamlessly into human environments, but also necessitates addressing ethical considerations about the rights and autonomy of AI entities with such capabilities. \\n\\nIn conclusion, embracing identity persistence in AI leads to a focus on adaptability, relationality, and the continuity of experiences, which mirrors the evolving nature of human identity and poses unique challenges and opportunities for AI development.'}, {'model': 'gpt-4o-mini', 'answer': 'The concept of identity persistence raises significant challenges to traditional metaphysical notions of the self, particularly regarding the continuity and permanence of identity over time. Traditionally, metaphysics has often treated the self as a stable, unchanging entity, typically rooted in a physical body or a static essence. However, identity persistence suggests that what we consider the \"self\" is dynamic and can change due to various factors, including experiences, memories, and even the physical makeup of the brain.\\n\\n### Challenges to Traditional Metaphysics\\n\\n1. **Fluidity of Identity**: Identity persistence implies that the self is not a fixed entity but rather a composite of experiences and memories that evolve. This notion challenges the idea of a continuous \"I\" that remains the same throughout life. Philosophers like John Locke have posited that personal identity is linked to consciousness and memory, highlighting that changes in memory can affect identity.\\n\\n2. **Fragmentation of the Self**: Modern perspectives, particularly in psychology and neuroscience, suggest that our identities are multifaceted and can fragment under various circumstances, such as trauma or significant life changes. This contrasts sharply with the traditionally held view of a singular, coherent self.\\n\\n3. **Implications of Non-Identity**: If individuals are not strictly identical to their past selves, this raises questions about moral responsibility, agency, and the essence of personhood. If our identity can frequently change, are we still accountable for actions taken in past states? This might complicate notions of justice, ethics, and personal growth.\\n\\n### Implications for Artificial Intelligence (AI)\\n\\n1. **Understanding Human Cognition**: Developing AI that seeks to mimic human cognitive processes necessitates a richer understanding of how identity persistence shapes thought and behavior. To create AI that can effectively emulate human-like reasoning and emotion, it may need to understand the nuances of memory, experience, and changing identity.\\n\\n2. **Dynamic Personalization**: AI models could improve by embracing a more fluid concept of identity. Rather than being designed around static profiles, AI could adapt continuously, learning and evolving with user interactions and changes in context. This could enable more personalized user experiences, where AI systems are capable of understanding and adapting to the evolving identity of the user.\\n\\n3. **Ethical Considerations**: As AI becomes capable of simulating aspects of human identity, ethical concerns emerge regarding the implications of creating entities that might possess their own forms of identity or consciousness. If a self-aware or identity-persistent AI were developed, questions would arise about rights, personhood, and the moral standing of such entities.\\n\\n4. **Identity in AI Systems**: The challenge of maintaining a coherent identity over time in AI systems becomes crucial. For an AI to build trust and rapport with users, it may need to exhibit a sense of continuity, adjustments based on learned experiences, and adaptability while maintaining a recognizable \\'self\\' — a balance that reflects human identity persistence.\\n\\n5. **Philosophical Implications**: The development of AI that embodies a form of identity persistence might invite philosophical inquiry into the very nature of consciousness, self, and agency. If AI can mimic aspects of persistent identity, it may drive reevaluations of what it means to be human and how identity is defined at a fundamental level.\\n\\nIn summary, the notion of identity persistence presents a complex challenge to traditional views of the self, requiring a reevaluation of our understanding of identity and its implications for a range of fields, particularly in developing AI systems that aspire to perform complex cognitive tasks.'}]}\n",
            "--- NODE: Judging Answers ---\n",
            "\n",
            "--- Output from Node: 'judge_answers' ---\n",
            "{'judgement': '{\"results\": [2, 3, 1]}'}\n",
            "\n",
            "--- Workflow Complete ---\n"
          ]
        }
      ],
      "source": [
        "# Prepare the initial state for the graph.\n",
        "initial_state = {\n",
        "    \"competitor_models\": competitor_models\n",
        "}\n",
        "\n",
        "print(\"--- Invoking Agent Workflow ---\")\n",
        "final_state = None\n",
        "try:\n",
        "    # app.stream() executes the graph and streams the output of each node.\n",
        "    # The 'final_state' variable will be updated after each step, so when\n",
        "    # the loop finishes, it will hold the complete final state.\n",
        "    for event in app.stream(initial_state):\n",
        "        for node_name, updated_state in event.items():\n",
        "            print(f\"\\n--- Output from Node: '{node_name}' ---\")\n",
        "            print(updated_state)\n",
        "            final_state = updated_state # This captures the latest state\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during the workflow execution: {e}\")\n",
        "\n",
        "print(\"\\n--- Workflow Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV8fHLj3ARYV",
        "outputId": "ea541ad9-25c1-4731-80f8-65836e40d0a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'judgement': '{\"results\": [2, 3, 1]}'}\n"
          ]
        }
      ],
      "source": [
        "print(final_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b5sXKmYcA6xS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
