{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Building an AI Agent with LangChain: A Detailed Step-by-Step Guide\n",
    "\n",
    "Welcome! This notebook provides a detailed, beginner-friendly guide to building your first AI Agent using the LangChain framework. An AI Agent can do more than just chat; it can use **tools** to interact with the outside world, look up information, and perform actions.\n",
    "\n",
    "### Lab Objectives:\n",
    "By the end of this lab, you will have:\n",
    "1.  **Set up** a Python environment with LangChain and connected it to an LLM (OpenAI).\n",
    "2.  **Created and tested custom tools** for the agent (Wikipedia Search and a Calculator).\n",
    "3.  **Built an Agent** that can intelligently decide which tool to use based on your question.\n",
    "4.  **Added Memory** to the agent so it can remember previous parts of your conversation.\n",
    "5.  **Observed the agent's reasoning process** step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary Python packages. We'll add `python-dotenv` to help manage our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (0.3.28)\n",
      "Requirement already satisfied: requests in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (1.1.1)\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.11.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (0.4.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (2.0.42)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-openai) (1.98.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.7.34)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from numexpr) (2.3.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/venvs/jupyterhub/venv_d3/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Using cached numexpr-2.11.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (403 kB)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-openai requests python-dotenv numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Set Up the OpenAI API Key\n",
    "\n",
    "This code loads your OpenAI API key from a `.env` file located in the same directory as the notebook. \n",
    "\n",
    "**Instructions for local setup:**\n",
    "1. Create a file named `.env` in the same folder as this notebook.\n",
    "2. Add the following line to the `.env` file, replacing the placeholder with your actual key:\n",
    "   `OPENAI_API_KEY=\"sk-YourActualKeyHere\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API Key loaded successfully from .env file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "dotenv_path = os.path.expanduser(\"~/dev.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âš ï¸ OpenAI API Key not found. Make sure you have a .env file in the same directory with your key.\")\n",
    "else:\n",
    "    # Set the key for the LangChain library to use\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    print(\"âœ… OpenAI API Key loaded successfully from .env file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Creating a Custom Tool\n",
    "\n",
    "Tools are functions that an agent can use to get information or perform actions outside of its own knowledge. Our first tool will be a simple function to search Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Define the Tool's Functionality\n",
    "\n",
    "Let's create a standard Python function that takes a topic and uses the Wikipedia API to fetch a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wiki_search(topic: str) -> str:\n",
    "    \"\"\"Fetches a summary from the English Wikipedia for a given topic.\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{topic}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'accept': 'application/json'})\n",
    "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        data = response.json()\n",
    "        return data.get(\"extract\", \"No summary available.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching data: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Test the Raw Function\n",
    "\n",
    "Before giving the function to our agent, let's make sure it works on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the wiki_search function...\n",
      "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the wiki_search function...\")\n",
    "summary = wiki_search(\"LangChain\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Wrap the Function as a LangChain Tool\n",
    "\n",
    "Now, we wrap our Python function in a LangChain `Tool` object. This is how the agent recognizes it.\n",
    "\n",
    "**This is a critical step!** The `description` field is what the agent's LLM brain reads to decide **when** to use this tool. A good description is essential for the agent to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "wiki_tool = Tool(\n",
    "    name=\"wiki_search\",\n",
    "    func=wiki_search, # The function the tool will execute\n",
    "    description=\"Use this tool to get a summary of any topic from Wikipedia. Input should be a search term.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Debugging)** Let's inspect the created tool to see its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: wiki_search\n",
      "Tool Description: Use this tool to get a summary of any topic from Wikipedia. Input should be a search term.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tool Name: {wiki_tool.name}\")\n",
    "print(f\"Tool Description: {wiki_tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Initializing the LLM\n",
    "\n",
    "The agent's \"brain\" is a Large Language Model (LLM). We'll instantiate the OpenAI LLM wrapper from LangChain. The `temperature` parameter controls the creativity of the model's responses (0.0 is very deterministic, 1.0 is very creative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Initialized.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0.2, # Lower temperature for more factual responses\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Creating and Running the Agent\n",
    "\n",
    "Now we assemble the components: the LLM (brain) and the tools (actions). We will use a **ReAct Agent**, which stands for \"Reasoning and Acting.\" It works by thinking step-by-step to decide what to do.\n",
    "\n",
    "Setting `verbose=True` is crucial for debugging, as it shows us the agent's chain of thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7799/1088013298.py:7: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# ZERO_SHOT_REACT_DESCRIPTION: defines a type of agent that decides which tool to use based only on the tool's description. It uses the ReAct framework to reason step-by-step to find an answer.\n",
    "# ZERO_SHOT: The agent doesn't need to see any examples of how to solve a problem. It figures out what to do on the fly based on your prompt and the tools it has.\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Define the list of tools our agent can use\n",
    "tools = [wiki_tool]\n",
    "\n",
    "# Create the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True # Set to True to see the agent's thought process\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Test the Agent\n",
    "\n",
    "Let's ask a question that requires looking up information. Watch the output from `verbose=True` to see how the agent thinks:\n",
    "1.  **Thought:** The agent thinks about whether it needs a tool.\n",
    "2.  **Action:** It decides to use the `wiki_search` tool.\n",
    "3.  **Action Input:** It determines what to search for.\n",
    "4.  **Observation:** It gets the result back from the tool.\n",
    "5.  **Thought:** It thinks about the final answer based on the observation.\n",
    "6.  **Final Answer:** It gives you the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: Tell me about the planet Mars.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7799/171037168.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should use the wiki_search tool to get information about Mars.\n",
      "Action: wiki_search\n",
      "Action Input: \"Mars\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMars is the fourth planet from the Sun. It is also known as the \"Red Planet\", because of its orange-red appearance. Mars is a desert-like rocky planet with a tenuous carbon dioxide atmosphere. At the average surface level the atmospheric pressure is a few thousandths of Earth's, atmospheric temperature ranges from âˆ’153 to 20Â Â°C and cosmic radiation is high. Mars retains some water, in the ground as well as thinly in the atmosphere, forming cirrus clouds, frost, larger polar regions of permafrost and ice caps, but no liquid surface water. Its surface gravity is roughly a third of Earth's or double that of the Moon. It is half as wide as Earth or twice the Moon, with a diameter of 6,779Â km (4,212Â mi), and has a surface area the size of all the dry land of Earth.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is really interesting information about Mars.\n",
      "Action: wiki_search\n",
      "Action Input: \"Mars\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMars is the fourth planet from the Sun. It is also known as the \"Red Planet\", because of its orange-red appearance. Mars is a desert-like rocky planet with a tenuous carbon dioxide atmosphere. At the average surface level the atmospheric pressure is a few thousandths of Earth's, atmospheric temperature ranges from âˆ’153 to 20Â Â°C and cosmic radiation is high. Mars retains some water, in the ground as well as thinly in the atmosphere, forming cirrus clouds, frost, larger polar regions of permafrost and ice caps, but no liquid surface water. Its surface gravity is roughly a third of Earth's or double that of the Moon. It is half as wide as Earth or twice the Moon, with a diameter of 6,779Â km (4,212Â mi), and has a surface area the size of all the dry land of Earth.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Mars is the fourth planet from the Sun and is known as the \"Red Planet\". It has a desert-like rocky surface, a tenuous carbon dioxide atmosphere, and retains some water. Its surface gravity is roughly a third of Earth's and it has a diameter of 6,779Â km (4,212Â mi).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mars is the fourth planet from the Sun and is known as the \"Red Planet\". It has a desert-like rocky surface, a tenuous carbon dioxide atmosphere, and retains some water. Its surface gravity is roughly a third of Earth\\'s and it has a diameter of 6,779\\xa0km (4,212\\xa0mi).'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about the planet Mars.\"\n",
    "print(f\"Executing query: {query}\")\n",
    "agent.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Adding Memory to the Agent\n",
    "\n",
    "By default, the agent is **stateless**â€”it forgets everything after answering a question. Let's add memory so we can have a real conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Memory object created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7799/3322927099.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# The memory_key must match the input variable in the agent's prompt\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "print(\"âœ… Memory object created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we re-initialize the agent, this time passing the `memory` object. We also change the `AgentType` to one that is optimized for conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent with memory created.\n"
     ]
    }
   ],
   "source": [
    "agent_with_memory = initialize_agent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, \n",
    "    memory=memory, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent with memory created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1: Test the Conversational Agent\n",
    "\n",
    "Let's test it with a follow-up question. The agent should now be able to use the context from the first question to understand the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: wiki_search\n",
      "Action Input: Ada Lovelace\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAugusta Ada King, Countess of Lovelace, also known as Ada Lovelace, was an English mathematician and writer chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Ada Lovelace was a pioneering mathematician and writer who is known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognize the potential of the machine beyond just pure calculation.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ada Lovelace was a pioneering mathematician and writer who is known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognize the potential of the machine beyond just pure calculation.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First question\n",
    "agent_with_memory.run(\"Who was Ada Lovelace?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: wiki_search\n",
      "Action Input: Ada Lovelace\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAugusta Ada King, Countess of Lovelace, also known as Ada Lovelace, was an English mathematician and writer chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Ada Lovelace lived from December 10, 1815 to November 27, 1852.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ada Lovelace lived from December 10, 1815 to November 27, 1852.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow-up question\n",
    "# The agent should know that \"she\" refers to Ada Lovelace from the previous turn.\n",
    "agent_with_memory.run(\"When did she live?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Debugging)** Let's inspect the memory buffer to see what the agent has remembered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who was Ada Lovelace?\n",
      "AI: Ada Lovelace was a pioneering mathematician and writer who is known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognize the potential of the machine beyond just pure calculation.\n",
      "Human: When did she live?\n",
      "AI: Ada Lovelace lived from December 10, 1815 to November 27, 1852.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Extending the Agent with a Calculator Tool\n",
    "\n",
    "An agent's power comes from having multiple tools. Let's add a calculator. LLMs are notoriously bad at math, so giving them a calculator tool is a common and effective pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calculator tool created.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "# LLMMathChain is a pre-built tool that is much safer than using Python's eval()\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "\n",
    "calc_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=llm_math_chain.run,\n",
    "    description=\"Use this tool to evaluate any mathematical expression.\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Calculator tool created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a new agent that has access to **both** the Wikipedia tool and the Calculator tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Multi-tool agent created.\n"
     ]
    }
   ],
   "source": [
    "multi_tool_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "tools_multi = [wiki_tool, calc_tool]\n",
    "\n",
    "multi_tool_agent = initialize_agent(\n",
    "    tools=tools_multi,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=multi_tool_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Multi-tool agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1: Test the Multi-Tool Agent\n",
    "\n",
    "Let's see if the agent can correctly choose the right tool for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Calculator\n",
      "Action Input: 128 * 7\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "128 * 7\u001b[32;1m\u001b[1;3m```text\n",
      "128 * 7\n",
      "```\n",
      "...numexpr.evaluate(\"128 * 7\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m896\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 896\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: 896 is the answer to 128 multiplied by 7. Is there anything else you would like to know?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'896 is the answer to 128 multiplied by 7. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should use the Calculator tool\n",
    "multi_tool_agent.run(\"What is 128 multiplied by 7?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: wiki_search\n",
      "Action Input: Alan Turing\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAlan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Alan Turing was a brilliant mathematician and computer scientist who made significant contributions to the field of theoretical computer science. He is often referred to as the father of theoretical computer science and is known for his work on the Turing machine, which is considered a model of a general-purpose computer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Alan Turing was a brilliant mathematician and computer scientist who made significant contributions to the field of theoretical computer science. He is often referred to as the father of theoretical computer science and is known for his work on the Turing machine, which is considered a model of a general-purpose computer.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should use the wiki_search tool\n",
    "multi_tool_agent.run(\"And who is Alan Turing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion & Next Steps\n",
    "\n",
    "Congratulations! You have successfully built a conversational AI agent that can use multiple tools to answer questions and perform tasks. \n",
    "\n",
    "#### Key Takeaways:\n",
    "1.  **Agents = LLM (Brain) + Tools (Actions)**: The core idea is to give a language model the ability to interact with external systems.\n",
    "2.  **Tool Descriptions are Crucial**: The agent decides which tool to use based on the `description` you provide. Make them clear and specific.\n",
    "3.  **Memory Enables Conversation**: Without memory, each query is isolated. Memory provides context for follow-up questions.\n",
    "4.  **`verbose=True` is Your Best Friend**: It's essential for understanding and debugging the agent's reasoning process."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building_an_AI_Agent_with_LangChain_Generic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
