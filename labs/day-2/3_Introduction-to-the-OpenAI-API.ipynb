{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Adventure in Agentic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a good practice to verify that the API key has been loaded correctly.\n",
    "# This check is for the OpenAI key, but you can adapt it for any other service (e.g., 'GOOGLE_API_KEY').\n",
    "# Note: Ollama runs locally and does not require an API key.\n",
    "\n",
    "import os\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    # Print only the first few characters for security.\n",
    "    print(f\"OpenAI API Key is set and starts with: {openai_api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please check the .env file and the setup guide.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core import for interacting with OpenAI's models.\n",
    "# LangChain uses this same structure to provide a unified interface for various LLMs (like Gemini, Claude, etc.).\n",
    "# This makes it easy to switch between different model providers.\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client.\n",
    "# This object is the main entry point for making API calls.\n",
    "# For other providers (like Google's Gemini or local models via Ollama), the instantiation would be different.\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All conversations with the API are structured as a list of message objects.\n",
    "# Each message has a 'role' ('user', 'assistant', or 'system') and 'content'.\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is 2+2?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's send the request to the model.\n",
    "# Model Selection: 'gpt-4o-mini' is a great choice for balancing cost, speed, and intelligence.\n",
    "# Other options include 'gpt-4o' (most powerful), 'gpt-4-turbo', or free local models via Ollama.\n",
    "# The model choice can significantly impact the response quality and cost.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # A powerful and cost-effective model\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# The response content is nested within the 'choices' list.\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a two-step conversation.\n",
    "# Step 1: Ask the LLM to generate a challenging question.\n",
    "\n",
    "question_prompt = \"Propose a hard, challenging question to assess someone's logical reasoning. Respond only with the question itself.\"\n",
    "messages = [{\"role\": \"user\", \"content\": question_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the request to get the question.\n",
    "# Using a slightly more powerful model might yield a more creative question.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "generated_question = response.choices[0].message.content\n",
    "\n",
    "print(f\"Generated Question:\\n{generated_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Now, use the generated question as a new prompt to get an answer.\n",
    "messages = [{\"role\": \"user\", \"content\": generated_question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to answer its own question.\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"Generated Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better readability in notebooks, we can render the output as Markdown.\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "This was a successful first step into the world of Agentic AI. The environment is set up and working correctly.\n",
    "\n",
    "Next, we can explore more complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A 3-Step Agentic Workflow\n",
    "\n",
    "Let's try a simple commercial application using a chain of LLM calls:\n",
    "1.  **Ideation**: Ask the LLM to identify a business sector ripe for an Agentic AI solution.\n",
    "2.  **Problem Discovery**: Ask the LLM to describe a specific, challenging pain point within that sector.\n",
    "3.  **Solution Proposal**: Ask the LLM to propose an Agentic AI system to solve that pain point.\n",
    "\n",
    "Below is a complete, working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Identify a business area ---\n",
    "prompt1 = \"Which business sector could significantly benefit from an Agentic AI solution? Respond with the sector name only.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt1}]\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "business_sector = response1.choices[0].message.content.strip()\n",
    "print(f\"1. Identified Sector: {business_sector}\\n\")\n",
    "\n",
    "# --- Step 2: Identify a pain point in that area ---\n",
    "prompt2 = f\"What is a major, unsolved pain point in the '{business_sector}' industry? Describe it clearly and concisely.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt2}]\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "pain_point = response2.choices[0].message.content.strip()\n",
    "print(f\"2. Identified Pain Point:\\n{pain_point}\\n\")\n",
    "\n",
    "# --- Step 3: Propose an Agentic AI solution ---\n",
    "prompt3 = f\"Considering the pain point in '{business_sector}': '{pain_point}'. Propose a high-level concept for an Agentic AI system that could solve this. Describe its key functions and agents.\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt3}]\n",
    "\n",
    "response3 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0.7 # Add some creativity to the solution\n",
    ")\n",
    "solution = response3.choices[0].message.content.strip()\n",
    "print(f\"3. Proposed Agentic AI Solution:\")\n",
    "display(Markdown(solution))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
