{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌐 Connecting AI Agents to External APIs: Search & Compute\n",
    "\n",
    "Welcome to this hands-on guide where we'll build a sophisticated AI agent that can connect to the real world. By integrating external APIs, we can empower our agent to answer questions about current events, perform complex calculations, and access structured data, overcoming the limitations of a standard LLM.\n",
    "\n",
    "### Lab Objectives:\n",
    "By the end of this lab, you will have:\n",
    "1.  **Acquired API keys** for Google Search (via SerpAPI) and Wolfram Alpha.\n",
    "2.  **Built custom tools** for each API, making them available to a LangChain agent.\n",
    "3.  **Assembled a multi-tool agent** that can intelligently choose between searching the web and performing computations.\n",
    "4.  **Observed the agent's reasoning** as it decides which tool is best for a given query.\n",
    "5.  **Added memory** to the agent to enable conversational follow-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setup and API Key Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Install Required Libraries\n",
    "\n",
    "We need to install the libraries for LangChain, OpenAI, SerpAPI, and `python-dotenv` for managing our keys. We will **not** use the `wolframalpha` library due to conflicts with notebook environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-openai google-search-results python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Set Up Your API Keys\n",
    "\n",
    "This agent requires **three** API keys to function. This code will first attempt to load them from a `.env` file in the current directory. If it can't find the file or the keys within it, it will securely prompt you to enter them.\n",
    "\n",
    "**To use a `.env` file (recommended for local use):**\n",
    "1.  Create a file named `.env` in the same directory as this notebook.\n",
    "2.  Add your keys like this:\n",
    "    ```\n",
    "    OPENAI_API_KEY=\"sk-YourKeyHere\"\n",
    "    SERPAPI_API_KEY=\"YourSerpApiKeyHere\"\n",
    "    WOLFRAM_ALPHA_APPID=\"YourWolframAppIdHere\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Keys are set for this session.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file, if it exists\n",
    "dotenv_path = os.path.expanduser(\"~/dev.env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "def get_api_key(key_name: str) -> str:\n",
    "    \"\"\"Gets an API key from environment variables or prompts the user for it.\"\"\"\n",
    "    key = os.getenv(key_name)\n",
    "    if not key:\n",
    "        print(f\"Could not find {key_name} in .env file.\")\n",
    "        key = getpass.getpass(f\"Please enter your {key_name}: \")\n",
    "    return key\n",
    "\n",
    "# Get all three API keys\n",
    "OPENAI_API_KEY = get_api_key(\"OPENAI_API_KEY\")\n",
    "SERPAPI_API_KEY = get_api_key(\"SERPAPI_API_KEY\")\n",
    "WOLFRAM_ALPHA_APPID = get_api_key(\"WOLFRAM_ALPHA_APPID\")\n",
    "\n",
    "# Set them as environment variables for the libraries to use\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['SERPAPI_API_KEY'] = SERPAPI_API_KEY\n",
    "os.environ['WOLFRAM_ALPHA_APPID'] = WOLFRAM_ALPHA_APPID\n",
    "\n",
    "print(\"✅ API Keys are set for this session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Creating the Google Search Tool\n",
    "\n",
    "This tool will allow our agent to search the live web to answer questions about recent events or topics not in its training data. We use SerpAPI as a convenient way to access Google Search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Define the Search Function\n",
    "\n",
    "This Python function takes a search query, sends it to the SerpAPI, and processes the results to return a clean, readable summary of the top hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "def google_search_func(query: str) -> str:\n",
    "    \"\"\"Performs a Google search using the SerpAPI and returns a summary of the top results.\"\"\"\n",
    "    print(f\"\\n{'='*10} DEBUG: GOOGLE SEARCH TOOL {'='*10}\")\n",
    "    print(f\"Performing Google Search for: '{query}'\")\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": os.environ['SERPAPI_API_KEY'],\n",
    "        \"num\": \"5\" # Request 5 results\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        organic_results = results.get(\"organic_results\", [])\n",
    "        \n",
    "        if not organic_results:\n",
    "            return \"No good search results found.\"\n",
    "        \n",
    "        summary_parts = []\n",
    "        for res in organic_results[:3]:\n",
    "            title = res.get(\"title\", \"No Title\")\n",
    "            snippet = res.get(\"snippet\", \"No Snippet\")\n",
    "            summary_parts.append(f\"Title: {title}\\nSnippet: {snippet}\")\n",
    "        \n",
    "        final_summary = \"\\n\\n---\\n\\n\".join(summary_parts)\n",
    "        return final_summary\n",
    "    except Exception as e:\n",
    "        return f\"Error during Google Search: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Wrap and Test the Tool\n",
    "\n",
    "Now we wrap our function in a LangChain `Tool` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Search Tool created.\n"
     ]
    }
   ],
   "source": [
    "google_tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    func=google_search_func,\n",
    "    description=\"Use this tool to find information about recent events, current affairs, or topics on the web. It is good for finding facts and summaries.\"\n",
    ")\n",
    "\n",
    "print(\"✅ Google Search Tool created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Creating the Wolfram Alpha Tool (Robust Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define the Wolfram Alpha Function using `requests`\n",
    "\n",
    "This function calls the Wolfram Alpha \"Full Results API\" endpoint. It requests the output in JSON format and then parses the response to find the most direct answer, usually found in the pod titled \"Result\" or \"Value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def wolfram_query_func_requests(query: str) -> str:\n",
    "    \"\"\"Sends a query to Wolfram Alpha's Full Results API and returns a plaintext answer.\"\"\"\n",
    "    print(f\"\\n{'='*10} DEBUG: WOLFRAM ALPHA REQUESTS TOOL {'='*10}\")\n",
    "    print(f\"Querying Wolfram Alpha for: '{query}'\")\n",
    "    \n",
    "    app_id = os.environ['WOLFRAM_ALPHA_APPID']\n",
    "    api_url = \"http://api.wolframalpha.com/v2/query\"\n",
    "    \n",
    "    params = {\n",
    "        \"input\": query,\n",
    "        \"format\": \"plaintext\",\n",
    "        \"output\": \"JSON\",\n",
    "        \"appid\": app_id\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, params=params)\n",
    "        response.raise_for_status() # Raise an error for bad status codes\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API response is complex. We need to parse it carefully.\n",
    "        query_result = data.get(\"queryresult\", {})\n",
    "        pods = query_result.get(\"pods\", [])\n",
    "        \n",
    "        if query_result.get(\"success\") and pods:\n",
    "            # Find the most relevant pod (usually 'Result' or 'Value')\n",
    "            for pod in pods:\n",
    "                if pod.get(\"primary\"): # The primary pod is usually the main answer\n",
    "                    subpods = pod.get(\"subpods\", [])\n",
    "                    if subpods:\n",
    "                        answer = subpods[0].get(\"plaintext\")\n",
    "                        print(f\"DEBUG: Found primary answer: {answer}\")\n",
    "                        return f\"Answer from Wolfram Alpha: {answer}\"\n",
    "            # If no primary pod, return the first pod's answer\n",
    "            if pods[0].get(\"subpods\"): \n",
    "                answer = pods[0][\"subpods\"][0].get(\"plaintext\")\n",
    "                print(f\"DEBUG: No primary pod, returning first result: {answer}\")\n",
    "                return f\"Answer from Wolfram Alpha ({pods[0].get('title')}): {answer}\"\n",
    "        \n",
    "        return \"Wolfram Alpha could not find an answer.\"\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"Error: Wolfram API returned status {http_err.response.status_code}. This often means your AppID is invalid.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Wrap and Test the New Tool\n",
    "\n",
    "Now, let's wrap our new, robust function as a LangChain tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing New Wolfram Alpha Tool Directly ---\n",
      "\n",
      "========== DEBUG: WOLFRAM ALPHA REQUESTS TOOL ==========\n",
      "Querying Wolfram Alpha for: 'What is the distance from the Earth to the Moon in kilometers?'\n",
      "DEBUG: Found primary answer: 403932 km (kilometers)\n",
      "\n",
      "--- End of Test ---\n",
      "\n",
      "Tool Output:\n",
      "Answer from Wolfram Alpha: 403932 km (kilometers)\n"
     ]
    }
   ],
   "source": [
    "wolfram_tool = Tool(\n",
    "    name=\"wolfram_alpha_query\",\n",
    "    func=wolfram_query_func_requests, # Use our new requests-based function\n",
    "    description=\"Use this tool for any questions requiring math, calculations, unit conversions, or scientific data. It is a computational and factual engine.\"\n",
    ")\n",
    "\n",
    "# Let's test the new tool directly\n",
    "print(\"--- Testing New Wolfram Alpha Tool Directly ---\")\n",
    "test_result = wolfram_tool.run(\"What is the distance from the Earth to the Moon in kilometers?\")\n",
    "print(\"\\n--- End of Test ---\")\n",
    "print(\"\\nTool Output:\")\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Building the Multi-Tool Agent\n",
    "\n",
    "Now we assemble our agent, giving it access to both the Google Search tool and our new, robust Wolfram Alpha tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-tool conversational agent created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8007/3383895787.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "/tmp/ipykernel_8007/3383895787.py:15: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "# Create the list of tools\n",
    "tools = [google_tool, wolfram_tool]\n",
    "\n",
    "# Initialize memory to allow for follow-up questions\n",
    "# Setting return_messages=True makes the memory object store and return the conversation history as a list of message objects instead of a single string.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# CONVERSATIONAL_REACT_DESCRIPTION is an agent type that combines the step-by-step reasoning of the ReAct framework with memory, allowing it to have a conversation and use tools based on their descriptions.\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"✅ Multi-tool conversational agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Test the Agent's Tool Selection\n",
    "\n",
    "Let's test our fully assembled agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8007/2629581940.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"What is 25% of 5820?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: wolfram_alpha_query\n",
      "Action Input: What is 25% of 5820?\u001b[0m\n",
      "========== DEBUG: WOLFRAM ALPHA REQUESTS TOOL ==========\n",
      "Querying Wolfram Alpha for: 'What is 25% of 5820?'\n",
      "DEBUG: Found primary answer: 1455\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer from Wolfram Alpha: 1455\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: 25% of 5820 is 1455. Is there anything else you would like to know?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'25% of 5820 is 1455. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test A: A question requiring computation\n",
    "agent.run(\"What is 25% of 5820?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: google_search\n",
      "Action Input: Who is the current prime minister of the United Kingdom?\u001b[0m\n",
      "========== DEBUG: GOOGLE SEARCH TOOL ==========\n",
      "Performing Google Search for: 'Who is the current prime minister of the United Kingdom?'\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Prime Minister of the United Kingdom\n",
      "Snippet: Prime Minister of the United Kingdom ; Incumbent Keir Starmer. since 5 July 2024 ; Government of the United Kingdom · Prime Minister's Office ...\n",
      "\n",
      "---\n",
      "\n",
      "Title: Prime Minister\n",
      "Snippet: Sir Keir Starmer became Prime Minister on 5 July 2024. Education. Keir attended Reigate Grammar School, before studying Law at the University of Leeds. He went ...\n",
      "\n",
      "---\n",
      "\n",
      "Title: Keir Starmer\n",
      "Snippet: Rishi Sunak · Prime Minister of the United Kingdom 2024–present, Incumbent. Minister for the Civil Service 2024–present. First Lord of the Treasury 2024–present.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The current prime minister of the United Kingdom is Keir Starmer. Is there anything else you would like to know?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current prime minister of the United Kingdom is Keir Starmer. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test B: A question requiring a web search\n",
    "agent.run(\"Who is the current prime minister of the United Kingdom?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSATIONAL_REACT_DESCRIPTION: Remember the previous interactions.\n",
    "agent.run(\"When hea was born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations! You've built a powerful, conversational AI agent that can leverage external APIs to answer a wide range of questions. By diagnosing the library issue and building a more robust tool with `requests`, you've also learned a valuable lesson in debugging and working with external services.\n",
    "\n",
    "#### Key Takeaways:\n",
    "1.  **Direct API Calls are a Powerful Debugging Tool**: When a library fails, testing the API directly with `requests` can quickly identify the root cause.\n",
    "2.  **Robustness over Convenience**: Sometimes, bypassing a convenient but problematic library in favor of a direct implementation is the best solution.\n",
    "3.  **Agents are Force Multipliers**: By connecting to APIs, agents can provide real-time, factual, and computationally accurate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Connecting_AI_Agents_to_External_APIs_Enhanced.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
