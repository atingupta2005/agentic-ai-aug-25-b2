{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Building a Model Evaluator with LangGraph\n",
    "\n",
    "This notebook refactors our previous model comparison script into a stateful, robust agent using LangGraph. By structuring the workflow as a graph, we gain better control, observability, and the ability to easily extend the process in the future.\n",
    "\n",
    "### Key Features:\n",
    "1.  **Stateful Agent**: The entire workflow is managed within a `StateGraph`.\n",
    "2.  **Modular Nodes**: Each logical step (generating a question, querying models, judging) is a separate, well-defined node.\n",
    "3.  **Configuration Driven**: Uses a `config.ini` file for settings.\n",
    "4.  **Visualization**: Displays a diagram of the agent's graph structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv openai langgraph ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration Loading\n",
    "\n",
    "**Purpose**: To import necessary libraries and load all settings from our external `config.ini` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import configparser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, Image\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    load_dotenv()\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    # Model settings\n",
    "    JUDGE_MODEL = config.get('Models', 'judge_model')\n",
    "    competitor_models_str = config.get('Models', 'competitor_models')\n",
    "    competitor_models = [model.strip() for model in competitor_models_str.split(',')]\n",
    "\n",
    "    print(\"Configuration loaded successfully!\")\n",
    "    print(f\"Competitor models: {competitor_models}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Agent's State\n",
    "\n",
    "**Purpose**: To define the data structure that will act as the agent's memory. This `GraphState` will be passed to every node in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represents the state of our model evaluation workflow.\"\"\"\n",
    "    question: str\n",
    "    competitor_models: List[str]\n",
    "    answers: List[dict]\n",
    "    judgement: str\n",
    "    error_message: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Agent's Nodes\n",
    "\n",
    "**Purpose**: To create the functions that will perform the actual work. Each function takes the current state as input and returns a dictionary with the values to update in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_node(state: GraphState):\n",
    "    \"\"\"Generates the challenge question.\"\"\"\n",
    "    print(\"--- NODE: Generating Question ---\")\n",
    "    request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation or preamble.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": request}]\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages)\n",
    "        question = response.choices[0].message.content\n",
    "        return {\"question\": question}\n",
    "    except Exception as e:\n",
    "        return {\"error_message\": f\"Failed to generate question: {e}\"}\n",
    "\n",
    "def query_models_node(state: GraphState):\n",
    "    \"\"\"Queries each competitor model with the challenge question.\"\"\"\n",
    "    print(\"--- NODE: Querying Competitor Models ---\")\n",
    "    question = state['question']\n",
    "    models_to_query = state['competitor_models']\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    current_answers = []\n",
    "    client = OpenAI()\n",
    "\n",
    "    for model_name in models_to_query:\n",
    "        print(f\"  - Querying {model_name}...\")\n",
    "        try:\n",
    "            response = client.chat.completions.create(model=model_name, messages=messages)\n",
    "            answer = response.choices[0].message.content\n",
    "            current_answers.append({\"model\": model_name, \"answer\": answer})\n",
    "        except Exception as e:\n",
    "            error_message = f\"Could not get response from {model_name}: {e}\"\n",
    "            print(f\"    ERROR: {error_message}\")\n",
    "            current_answers.append({\"model\": model_name, \"answer\": error_message})\n",
    "    \n",
    "    return {\"answers\": current_answers}\n",
    "\n",
    "def judge_answers_node(state: GraphState):\n",
    "    \"\"\"Uses the judge model to evaluate and rank the answers.\"\"\"\n",
    "    print(\"--- NODE: Judging Answers ---\")\n",
    "    question = state['question']\n",
    "    answers = state['answers']\n",
    "    \n",
    "    all_answers_text = \"\"\n",
    "    for index, response in enumerate(answers):\n",
    "        all_answers_text += f\"# Response {index+1} ({response['model']})\\n{response['answer']}\\n---\\n\"\n",
    "\n",
    "    judge_prompt = f'You are an impartial judge in a competition between multiple AI models. Your task is to evaluate each model\\'s response to a question based on clarity, depth, and accuracy. The question was: \"{question}\". Here are the responses:\\n\\n{all_answers_text}Please rank them from best to worst and respond with a JSON object. The object should have a single key, \"results\", which is a list of the competitor numbers (as integers) in ranked order. Example: {{\"results\": [2, 1, 3]}}'\n",
    "    messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(model=JUDGE_MODEL, messages=messages, response_format={\"type\": \"json_object\"})\n",
    "        judgement = response.choices[0].message.content\n",
    "        return {\"judgement\": judgement}\n",
    "    except Exception as e:\n",
    "        return {\"error_message\": f\"Failed to get judgement: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct and Visualize the Graph\n",
    "\n",
    "**Purpose**: To wire the nodes together into a coherent workflow and visualize the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add the nodes to the graph\n",
    "workflow.add_node(\"generate_question\", generate_question_node)\n",
    "workflow.add_node(\"query_models\", query_models_node)\n",
    "workflow.add_node(\"judge_answers\", judge_answers_node)\n",
    "\n",
    "# Define the edges that connect the nodes\n",
    "workflow.add_edge(START, \"generate_question\")\n",
    "workflow.add_edge(\"generate_question\", \"query_models\")\n",
    "workflow.add_edge(\"query_models\", \"judge_answers\")\n",
    "workflow.add_edge(\"judge_answers\", END)\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"LangGraph workflow compiled successfully!\")\n",
    "\n",
    "# Display the graph visualization\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execute the Agent Workflow\n",
    "\n",
    "**Purpose**: To run our newly created LangGraph agent. We prepare the initial state and then invoke the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial state for the graph.\n",
    "initial_state = {\n",
    "    \"competitor_models\": competitor_models\n",
    "}\n",
    "\n",
    "print(\"--- Invoking Agent Workflow ---\")\n",
    "final_state = None\n",
    "try:\n",
    "    # app.stream() executes the graph and streams the output of each node as it runs.\n",
    "    for event in app.stream(initial_state):\n",
    "        for node_name, updated_state in event.items():\n",
    "            print(f\"\\n--- Output from Node: '{node_name}' ---\")\n",
    "            print(updated_state)\n",
    "            final_state = updated_state # Keep track of the last known state\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the workflow execution: {e}\")\n",
    "\n",
    "print(\"\\n--- Workflow Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Final Rankings\n",
    "\n",
    "**Purpose**: To parse the final state from the graph, extract the judge's JSON response, and present the results in a clean, human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_state and final_state.get('judgement'):\n",
    "    results_json = final_state['judgement']\n",
    "    try:\n",
    "        results_dict = json.loads(results_json)\n",
    "        ranks = results_dict[\"results\"]\n",
    "        \n",
    "        display(Markdown(\"## Final Rankings\"))\n",
    "        for rank_index, competitor_number in enumerate(ranks):\n",
    "            # The competitor number is 1-based, so subtract 1 for the list index\n",
    "            competitor_name = final_state['answers'][int(competitor_number)-1]['model']\n",
    "            display(Markdown(f\"**Rank {rank_index+1}:** `{competitor_name}`\"))\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:\n",
    "        print(f\"\\nError parsing the judge's response. Please check the raw JSON output. Error: {e}\")\n",
    "        print(f\"Raw JSON: {results_json}\")\n",
    "elif final_state and final_state.get('error_message'):\n",
    "    print(f\"\\nWorkflow ended with an error: {final_state['error_message']}\")\n",
    "else:\n",
    "    print(\"\\nNo results to display. The workflow may not have completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
